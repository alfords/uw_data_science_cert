y_cdf2 = sapply(x_seq, function(x){
sum(dist_b<x)/length(dist_b)
})
k_s_stat = max(abs(y_cdf1-y_cdf2))
return(k_s_stat)
}
##----Repeat N Times-----
N = 1000
k_s_rep = sapply(1:N, function(i){
dist_a = rnorm(100,mean=0,sd=1)
dist_b = rnorm(100,mean=0,sd=1)
return(ks_stat(-3, 3, dist_a, dist_b))
})
hist(k_s_rep, breaks=30, freq=FALSE, xlab = 'K-S statistic',
main = 'Histogram of k-s statistic')
lines(density(k_s_rep))
##----Empirical Two Tailed KS test-----
# Alternative hypothesis is that the k-s- statistic
#  is greater than the "expected value".
dist1 = rnorm(1000, mean=0.05, sd = 1)
dist2 = rnorm(1000, mean=0, sd = 1)
k_s_stat = ks_stat(-5,5, dist1, dist2)
dist1 = rnorm(1000, mean=0.05, sd = 1)
dist2 = rnorm(1000, mean=0, sd = 1)
# Create k-s statistic function
ks_stat = function(x_min,x_max, dist_a, dist_b){
x_seq = seq(x_min,x_max,len=1000)
y_cdf1 = sapply(x_seq, function(x){
sum(dist_a<x)/length(dist_a)
})
y_cdf2 = sapply(x_seq, function(x){
sum(dist_b<x)/length(dist_b)
})
k_s_stat = max(abs(y_cdf1-y_cdf2))
return(k_s_stat)
}
dist1 = rnorm(1000, mean=0.05, sd = 1)
dist2 = rnorm(1000, mean=0, sd = 1)
k_s_stat = ks_stat(-5,5, dist1, dist2)
# What should the distribution be?
# i.e. what is the expected value?
k_s_hypothesis = sapply(1:500, function(i){
dist_a = rnorm(1000,mean=0,sd=1)
dist_b = rnorm(1000,mean=0,sd=1)
return(ks_stat(-3, 3, dist_a, dist_b))
})
hist(k_s_hypothesis, breaks=30, xlab = 'K-S statistic',
main = 'Histogram of k-s statistic')
##-----Shapiro-Wilk's Test----
dist_a = rnorm(100,mean=0,sd=1)
shapiro.test(dist_a)
# Can NOT reject null that dist_a is from a normal population.
qqnorm(dist_a, pch=16)
abline(0,1, lwd=2, col="red")
# #########################################################################
# Assignment 2: Monty Hall Simulation
# Method for Data Analysis
#
# Author: Josh Jensen
#
# Tasks:
# – Explore auto price data set.
# – Write R program that shows/illustrates 3 key takeaways of your
# choosing from exploring the data.
# #########################################################################
# Problem:
# Suppose you're on a game show, and you're given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what's behind the doors, opens another door, say No. 3, which has a goat. He then says to you, "Do you want to pick door No. 2?" Is it to your advantage to switch your choice?
#
### Set up
#
# load libraries and set working drive
library(dplyr)
library(ggplot2)
library(reshape2)
#
### Run Monty Hall Simulation
#
# Generate data frame of n obs, with random initial door choices & doors with car behind
# note: runif will not generate either of the extreme values unless max = min or max-min is small compared to min
simMontyHall <- function(n = 10000) {
df <- data.frame(door_choice = trunc(runif(n, min = 1, max = 4)),   # generates a uniform random number of 1, 2, or 3
door_with_car = trunc(runif(n, min = 1, max = 4))  # generates a uniform random number of 1, 2, or 3
)
# create boolean columns for both switch & stay strategies
df$stay_win <- ifelse(df$door_choice == df$door_with_car, T, F)
# note: since Monty Hall will never reveal the car, a switching strategy will win as long as the car was not behind the original choice
df$switch_win <- ifelse(df$door_choice != df$door_with_car, T, F)
df
}
# run with 1M observations
df <- simMontyHall(1000000)
#
### Summarize Results
#
# create & print summary df
summary_df <- summarise(df, stay_prob = mean(stay_win),
stay_var = var(stay_win),
switch_prob = mean(switch_win),
switch_var = var(switch_win))
melt(summary_df)
View(summary_df)
library(ggplot2)
mpg <- mtcars
library(ggplot2)
mpg <- mtcars
View(mpg)
mpg
df <- mpg
table(df$fl)
ggplot(df, aes(fl)) +
geom_bar()
geom_histogram()
ggplot(df, aes(fl)) +
geom_histogram()
ggplot(df, aes(fl)) +
geom_bar()
source('~/.active-rstudio-document', echo=TRUE)
### ------------------------------------------------
## Model diagnostics with coda package
##
## Set up the data set as a regression problem
require('rjags')
N <- 1000
x <- 1:N
epsilon <- rnorm(N, 0, 1)
y <- x + epsilon
require('rjags')
require('rjags')
N <- 1000
x <- 1:N
epsilon <- rnorm(N, 0, 1)
y <- x + epsilon
## Run the jags model
jags.mod.reg <- jags.model('example.bug',
data = list('x' = x,
'y' = y,
'N' = N),
n.chains = 4,
n.adapt = 1000)
## Compute some samples
samples <- coda.samples(jags.mod.reg,
c('a', 'b'),
1000)
plot(samples) # Plot the result
summary(samples) # Summary statistics
cumuplot(samples) # Cumulative mean for each chain
gelman.plot(samples) # Gelman convergence plot
## Look at the autocorrelation of the chain
autocorr.diag(samples)
autocorr.plot(samples)
## Load the data
require(mlbench)
data(HouseVotes84)
install.packages("mlbench")
##
## Load the data
require(mlbench)
data(HouseVotes84)
View(HouseVotes84)
## Examine the data ste
require(ggplot2)
Map(function(x, y)
ggplot(HouseVotes84, aes_string(x)) +
geom_bar() +
facet_grid(. ~ Class) +
ggtitle(y),
list('V1', 'V2', 'V3', 'V4', 'V5'),
list(' handicapped-infants',
'water-project-cost-sharing',
'adoption-of-the-budget-resolution',
'physician-fee-freeze',
'el-salvador-aid'))
## Compute the model
require(e1071)
model <- naiveBayes(Class ~ ., data = HouseVotes84)
## Look at the results for the first 10 legistators
party = predict(model, HouseVotes84[1:10,])
nums = predict(model, HouseVotes84[1:10,], type = "raw")
data.frame(party = HouseVotes84$Class[1:10], predicted = party, Democrat = nums[,1], Republican = nums[,2])
pred <- predict(model, HouseVotes84)
table(pred, HouseVotes84$Class)
table(pred
)
table(pred, HouseVotes84$Class) / length(pred)
table(pred, HouseVotes84$Class)
table(pred, HouseVotes84$Class) / length(pred)
model <- naiveBayes(Class ~ ., data = HouseVotes84, laplace = 3)
pred <- predict(model, HouseVotes84[,-1])
table(pred, HouseVotes84$Class)
table(pred, HouseVotes84$Class) / length(pred)
for(i in c(3, 5, 9, 17)){
model <- naiveBayes(Class ~ ., data = HouseVotes84[, 1:i], laplace = 3)
pred <- predict(model, HouseVotes84[,2:i])
print(table(pred, HouseVotes84$Class))
}
Income = read.csv('Adult Census Income Binary Classification dataset.csv',
header = TRUE)
library(LearnBayes)
##
library(LearnBayes)
## I think the chance of rain is 0.2 with
## with a probability at the 75% point of 0.28
## Compute my Beta prior
beta.par <- beta.select(list(p=0.5, x=0.2), list(p=0.75, x=.28))
beta.par ## The parameters of my Beta distribution
beta.post.par <- beta.par + c(6 + 6 + 12 + 12, 4 + 4 + 8 + 8)
post.sample <- rbeta(10000, beta.post.par[1], beta.post.par[2])
par(mfrow = c(1,2))
quants = quantile(post.sample, c(0.05, 0.95))
breaks = seq(min(post.sample), max(post.sample), length.out = 41)
hist(post.sample, breaks = breaks,
main = 'Distribution of samples \n with 90% HDI',
xlab = 'Sample value',
ylab = 'Density')
abline(v = quants[1], lty = 3, col = 'red', lwd = 3)
abline(v = quants[2], lty = 3, col = 'red', lwd = 3)
qqnorm(post.sample)
par(mfrow = c(1,1))
quants
predplot(beta.par, 6 + 8 + 12 + 12, 4 + 2 + 8 + 8)
beta.par ## The parameters of my Beta distribution
## next 60 trials?
n <- 60
s <- 0:n
pred.probs <- pbetap(beta.par, n, s)
plot(s, pred.probs, type="h",
main = 'Probability distribution of success in trail',
xlab = 'Successes')
discint(cbind(s, pred.probs), 0.90)
discint(cbind(s, pred.probs), 0.90)
pred.probs
sum(pred.probs[1:9]
)
sum(pred.probs[1:9])
pred.probs[1:9]
library(LearnBayes)
beta_prior <- beta.select(list(p=.5, x=.1), list(p=.75, x=.3))
beta_prior
predplot(beta.par, 6 + 8 + 12 + 12, 4 + 2 + 8 + 8)
beta_post <- beta_prior + c(2 + 4 + 1, 18 + 16 + 19)
pred_probs <- pbetap(beta_post, n, s)
plot(s, pred_probs, type = "h"
, main = "Probability distribution of success in trial"
, xlab = "successes")
pred_probs
sum(pred_probs[:4])
sum(pred_probs[1:4])
sum(pred_probs[1:3])
sum(pred_probs[1:2])
length(pred_probs)
sum(pred_probs[18:length(pred_probs)])
sum(pred_probs[16:length(pred_probs)])
sum(pred_probs[14:length(pred_probs)])
sum(pred_probs[15:length(pred_probs)])
sum(pred_probs[1:3])
# 3
sum(pred_probs[15:length(pred_probs)])
# 15
n <- 100
s <- 0:n
pred_probs <- pbetap(beta_post, n, s)
plot(s, pred_probs, type = "h"
, main = "Probability distribution of success in trial"
, xlab = "successes")
sum(pred_probs[1:3])
# calc posterior beta distribution
beta_post <- beta_prior + c(2 + 4 + 1, 18 + 16 + 19)
# random sample 100,000 obs
post_sample <- rbeta(100000, beta_post[1], beta_post[2])
# plot
par(mfrow = c(1, 2))
quants <- quantile(post_sample, c(0.05, 0.95))
breaks <- seq(min(post_sample), max(post_sample), length.out = 50)
hist(post_sample, breaks = breaks,
main = "Distribution of sample \n with 90% HDI"
, xlab = "sample value"
, ylab = "density")
abline(v = quants[1], lty = 3, col = "red", lwd = 3)
abline(v = quants[2], lty = 3, col = "red", lwd = 3)
qqnorm(post_sample)
quants
quantile(post_sample, c(.1, .3))
beta_prior
prior_sample <- rbeta(100000, beta_prior[1], beta_prior[2])
quantile(prior_sample, c(.1, .3))
quantile(prior_sample, c(.1, .3))
qbeta(c(.1, .3), beta_prior[1], beta_prior[2])
qbeta(c(.1, .3), beta_prior[1], beta_prior[2])
dbeta(c(.1, .3), beta_prior[1], beta_prior[2])
pbeta(c(.1, .3), beta_prior[1], beta_prior[2])
pbeta(c(.1, .3), beta_post[1], beta_post[2])
triplot(beta_prior, c(2,18), where = 'topright')
par(mfrow = c(1, 1))
triplot(beta_prior, c(2,18), where = 'topright')
beta_prior + c(2,18)
triplot(beta_prior, c(2 + 4, 18 + 16), where = 'topright')
beta_prior + c(2 + 4, 18 + 16)
# Now observe 1 texting out of 20 drivers
triplot(beta_prior, c(2 + 4 + 1, 18 + 16 + 19), where = 'topright')
beta_prior + c(2 + 4 + 1, 18 + 16 + 19)
triplot(beta_prior, c(2 + 4 + 1, 18 + 16 + 19), where = 'topright')
par(mfrow = c(1, 2))
quants <- quantile(post_sample, c(0.05, 0.95))
breaks <- seq(min(post_sample), max(post_sample), length.out = 50)
hist(post_sample, breaks = breaks,
main = "Distribution of sample \n with 90% HDI"
, xlab = "sample value"
, ylab = "density")
abline(v = quants[1], lty = 3, col = "red", lwd = 3)
abline(v = quants[2], lty = 3, col = "red", lwd = 3)
qqnorm(scale(post_sample))
par(mfrow = c(1, 1))
# plot
par(mfrow = c(1, 2))
quants <- quantile(post_sample, c(0.05, 0.95))
breaks <- seq(min(post_sample), max(post_sample), length.out = 50)
hist(post_sample, breaks = breaks,
main = "Distribution of sample \n with 90% HDI"
, xlab = "sample value"
, ylab = "density")
abline(v = quants[1], lty = 3, col = "red", lwd = 3)
abline(v = quants[2], lty = 3, col = "red", lwd = 3)
qqnorm(scale(post_sample))
par(mfrow = c(1, 1))
# plot
par(mfrow = c(1, 2))
quants <- quantile(post_sample, c(0.05, 0.95))
breaks <- seq(min(post_sample), max(post_sample), length.out = 50)
hist(post_sample, breaks = breaks,
main = "Distribution of sample \n with 90% HDI"
, xlab = "sample value"
, ylab = "density")
abline(v = quants[1], lty = 3, col = "red", lwd = 3)
abline(v = quants[2], lty = 3, col = "red", lwd = 3)
qqnorm(scale(post_sample))
abline(a=0, b=1, col = 'red')
par(mfrow = c(1, 1))
options(repos = c(CRAN = "http://cran.rstudio.com"))
install.packages("dplyr")
install.packages("magrittr")
install.packages("tidyr")
install.packages("readr")
install.packages("stringr")
library(dplyr)
library(magrittr)
library(tidyr)
library(readr)
library(stringr)
##Make new variable called data and choose and read cosmos csv
data <- file.choose()
data <- read_csv(data)
data <- read_csv(data)
data <- file.choose()
data <- read_csv(data)
##Load libraries
library(dplyr)
library(magrittr)
library(tidyr)
library(readr)
library(stringr)
data <- read_csv(data)
newData <- data %>% filter(Property == "_RollUp", Level == 1)
newData <- newData %>% group_by(Property) %>% summarise_each(funs(sum), CountPerformedMonthlyIntentional)
newData1 <- data %>% filter(Property == "DS", Level == 2)
newData1 <- newData1 %>% group_by(Property) %>% summarise_each(funs(sum), CountPerformedMonthlyIntentional)
##Make new variable called regions and choose and read regions csv
regions <- file.choose()
memory.limit()
data <- read.csv("C:/Users/jjensen/Dropbox/R/Putting the R in PointmaRc/cosmos-dataset.csv")
data <- read_csv("C:/Users/jjensen/Dropbox/R/Putting the R in PointmaRc/cosmos-dataset.csv")
data <- read_csv("cosmos-dataset.csv")
library(dplyr)
# load libraries
if (!require('dplyr',character.only = TRUE))
{
install.packages('dplyr',dep=TRUE)
}
if (!require('readr',character.only = TRUE))
{
install.packages('readr',dep=TRUE)
}
library(dplyr)
library(readr)
rownames(installed.packages())
?stl
library(dplyr)
library(readr)
'low'
as.integer('low')
1 > 'low'
1 < 'low'
1 < NA
1 > NA
0 < 'low'
-1 < 'low'
1>'a'
'a'>1
'a'<1
'a'<0
'a'<100
'a'<1
'a'>1
'a'>inf
'a'>Inf
'a'==Inf
'a' > 'b'
'b' > 'a'
'aa' > 'ab'
'aa' < 'ab'
setwd(choose.dir())
# set working directory
setwd('C:/Users/jjensen/Dropbox/UW - Data Science Cert/2 - Methods for Data Analysis/final_project')
# load libraries
library(dplyr)
library(readr)
library(lubridate)
library(reshape2)
library(xts)
library(tseries)
library(forecast)
library(ggplot2)
# read in data
# https://data.seattle.gov/api/views/kzjm-xkqj/rows.csv?accessType=DOWNLOAD
# calls911_log <- read_csv('https://data.seattle.gov/api/views/kzjm-xkqj/rows.csv?accessType=DOWNLOAD')
calls911_log <- read_csv('Seattle_Real_Time_Fire_911_Calls.csv')
# first glance inspection
head(calls911_log, n=15)
str(calls911_log)
# deal with first row
calls911_log <- calls911_log[-1,]
head(calls911_log)
# standardize column names
colnames(calls911_log) <- gsub(' ', '_', tolower(colnames(calls911_log)))
# check that datetime has no nas
sum(is.na(calls911_log$datetime))
# all good
# convert datetime from a character to a POSIX date-time
calls911_log$dt <- mdy_hms(calls911_log$datetime)
calls911_log[is.na(calls911_log$dt),]$dt <- as.POSIXct(ymd(gsub('T.*','',calls911_log[is.na(calls911_log$dt),]$datetime)))
# final look at the structure
str(calls911_log)
#
### Transform to time series (month, day, & hour)
#
# check if incident_numbers repeat
length(unique(calls911_log$incident_number))
# they do
# create hour, date, day, month, & year dimensions
calls911_log$hour <- hour(calls911_log$dt)
calls911_log$date <- as.Date(calls911_log$dt)
calls911_log$day <- day(calls911_log$dt)
calls911_log$month <- month(calls911_log$dt)
calls911_log$year <- year(calls911_log$dt)
# group bys
calls911_month <- calls911_log %>%
group_by(year, month) %>%
summarise(date = min(date), days = n_distinct(day), count = n()) %>%
arrange(date)
calls911_date <- calls911_log %>%
count(date) %>%
arrange(date)
calls911_hour <- calls911_log %>%
count(date, hour) %>%
arrange(date, hour) %>%
filter(!is.na(date))
# turn date into a time series
ts_date <- xts(calls911_date$n, order.by = calls911_date$date)
plot(ts_date)
# check out days
dcast(calls911_month, year ~ month, value.var = 'days')
# appears that time series is not represented consistently until 10/11
# create month ts
df <- filter(calls911_month, date >= '2011-11-01' & date <= '2016-04-01')
ts_month <- ts(df$count, start = (2011 + (11 - 1)/12), deltat = 1/12)
plot(ts_month)
#
### Predicting monthly volumes
#
# compare plots with different transformations
par(mfrow=c(3,1))
plot(ts_month)
plot(log(ts_month))
plot(diff(log(ts_month)))
par(mfrow=c(1,1))
# plot acf & pacf
acf_plots <- function(ts) {
par(mfrow=c(2,1))
acf(ts, main = "ACF")
pacf(ts, main = "PACF")
par(mfrow=c(1,1))
}
acf_plots(ts_month)
lag.plot(ts_month)
lag.plot(ts_month, lags = 9)
lag.plot(ts_month, lags = 9, do.lines = F)
lag.plot(log(ts_month), lags = 9, do.lines = F)
lag.plot(diff(log(ts_month)), lags = 9, do.lines = F)
log(.5)
*.5
log(.5)*.5
log2(.5)
log2(.5)*.5
log2(1)
log2(.1)
log2(.1) + log2(.9)
log2(.0001)
log2(.0000000000001)
log2(.00000000000000000000000000001)
log2(.5)
log2(1)
log2(.75)
log2(.99999)
