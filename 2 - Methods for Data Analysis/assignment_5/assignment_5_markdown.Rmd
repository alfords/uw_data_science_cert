---
title: "Assignment 5: Linear Regression Modelling"
subtitle: "Univeristy of Washington - Data Science Certification - Method for Data Analysis"
author: "Josh Jensen"
date: "August 3, 2016"
output: html_document
---

### Assignment

Construct and evaluate a linear model of automotive price:

- Model price by engine size, curb weight, and city mpg 
- Evaluate the significance of the model coefficients from the model summary
- Evaluate the performance of the model fit using boththe diagnostic plots and the model summary
- Test normality of residuals (e,g, SW test)


### Conclusions

We demonstrate that the following model is an effective model for modeling car price:

`log(price) ~ log(engine_size) + sqrt(curb_weight) + log(city_mpg) + engine_size:curb_weight + curb_weight:city_mpg`



### Set up & cleaning

First lets load libraries and data.

``` {r message=FALSE}
# load libraries and set working drive
library(dplyr)
library(readr)
library(ggplot2)
library(GGally)

# read in data
auto_prices <- read_csv('Automobile price data _Raw_.csv', na = c('','?')) 
```

Clean data set to make it easier to work with and inspect.

``` {r}
# rename columns
colnames(auto_prices) <- gsub('-','_', colnames(auto_prices))

# subset down to columns of interest
# price, engine size, curb weight, and city mpg 
vars <- c("engine_size", "curb_weight", "city_mpg")
auto_prices <- select(auto_prices, price, one_of(vars))

# create log price
auto_prices$log_price <- log(auto_prices$price)

# inspection of structure
str(auto_prices)
```



### Exploratory analysis of independent variable transformations

Now our objective is to determine suitable transformations of the three independent variables. 

To start lets take a look at a simple pairs plot. The `GGally` library's `ggpairs` plot provides an information rich view. 

``` {r message=FALSE, warning=FALSE}
ggpairs(auto_prices)
```

Now lets look at the pair plot of logged variables.

``` {r message=FALSE, warning=FALSE}
ggpairs(log(auto_prices))
```

It appears that generally under a log transform that the relationships are more linear and have less of a heteroskedasisty concern. However, lets evaluate this and other transforms side by side for each variable.

First as to facilitate this, we transform the data frame into a shape that lends itself easily to filtering and faceting.

``` {r}
auto_prices <- mutate(auto_prices, id = row_number())

ap_raw <- auto_prices %>% select(one_of(vars))
ap_raw <- mutate(ap_raw, id = row_number(), transform = "raw")

ap_log <- log(auto_prices) %>% select(one_of(vars))
ap_log <- mutate(ap_log, id = row_number(), transform = "log")

ap_sqrt <- sqrt(auto_prices) %>% select(one_of(vars))
ap_sqrt <- mutate(ap_sqrt, id = row_number(), transform = "square root")

ap_square <- data.frame(auto_prices^2) %>% select(one_of(vars))
ap_square <- mutate(ap_square, id = row_number(), transform = "squared")

ap_transformed <- bind_rows(ap_raw, ap_log, ap_sqrt, ap_square)
ap_transformed <- left_join(ap_transformed,
                            select(auto_prices, id, log_price),
                            by = "id")
```

Now we choose the outcome variable is log price, as it is more normally distributed as discussed in other assignments. 

Now against log price we will evaluate transforms (raw, log, square root, and square) by looping through all variables and transforms. For each variable we will evaluate a simple OLS model for each transform.

``` {r}
for (i in 1:length(vars)) {
  writeLines(paste("\n\n\n*****", vars[i], "*****"))
  
  temp <- select_(ap_transformed, vars[i], "log_price", "transform")
  temp <- na.omit(temp)
    
  p <- ggplot(temp, aes_string(y="log_price", x=vars[i])) +
    geom_point() +
    geom_smooth(method = lm) +
    facet_grid(. ~ transform, scales = "free_x") +
    ggtitle(paste("Transforms of",vars[i]))
  
  print(p)
  
  transforms <- unique(temp$transform)
  
  for (j in 1:length(transforms)) {
    writeLines(paste("\n\n\nSIMPLE OLS MODEL EVAL FOR:", transforms[j]))
    
    temp_filtered <- filter(temp, transform == transforms[j])
    
    m <- lm(as.formula(paste("log_price ~",vars[i])), temp_filtered)
    print(paste("R^2 =",summary(m)$r.squared))
    print("")
    print(summary(m)$coefficients)
  }
}
```

Based on comparing $R^2$ results across all OLS models and verifying that the coefficient is statistically significant, the transform of each variable that explains the most variance in log price is:

- engine size -> log
- curb weight -> square root
- city mpg    -> log



### Model price by engine size, curb weight, and city mpg 

First as point of comparison lets run an untransformed model. 

``` {r}
ols_baseline <- lm(price ~ engine_size + curb_weight + city_mpg, auto_prices)
summary(ols_baseline)
```

Lets also establish a baseline using log price.

``` {r}
ols_baseline2 <- lm(log(price) ~ engine_size + curb_weight + city_mpg, auto_prices)
summary(ols_baseline2)
```

Now lets apply the identified transformations to evaluate the fully defined transforms.

``` {r}
ols_transformed <- lm(log(price) ~ log(engine_size) + sqrt(curb_weight) + log(city_mpg), auto_prices)
summary(ols_transformed)
```



### Evaluate residuals

First lets look at diagnostic plots.

``` {r}
plot(ols_transformed)
```

Q-Q plot looks like there may be some deviance from normality in the residuals. So lets test for normality in the residuals using the Kolmogorov-Smirnov test and the Shapiro-Wilks test.

``` {r warning=FALSE}
# get residuals
residuals <- ols_transformed$residuals
# z normalize
residuals <- (residuals - mean(residuals)) / sd(residuals)

# Kolmogorov-Smirnov on residuals
ks.test(residuals, pnorm)
```

The Kolmogorov-Smirnov test on the models residuals at a confidence level of 95% **fails to reject** null hypothesis of residuals being normally distributed. Lets also confirm using the more strict Shapiro-Wilks test.

``` {r warning=FALSE}
# Shapiro-Wilks test on residuals
shapiro.test(residuals)
```

The Shapiro-Wilks test on the models residuals at a confidence level of 95% **rejects** the null hypothesis of residuals being normally distributed.



### Final model

Since the residuals did not pass both tests, heteroskedasticity may exist. This may exist because some interaction effect between terms was not accounted for. Now lets try adding in a full set of interaction terms.

``` {r}
# run model
ols_interaction <- lm(log(price) ~ log(engine_size) + sqrt(curb_weight) + log(city_mpg) + 
                engine_size:curb_weight + engine_size:city_mpg + curb_weight:city_mpg + engine_size:curb_weight:city_mpg
                , auto_prices)

# evaluate
summary(ols_interaction)
```

We notice that the $R^2$ is higher in the model with the interaction term.

After iteration, we notice that including interaction terms just between `engine_size:curb_weight` and `curb_weight:city_mpg` provides the cleanest model in terms of optimizing coefficient significance while maintaining $R^2$.

``` {r}
# run model
ols_final <- lm(log(price) ~ log(engine_size) + sqrt(curb_weight) + log(city_mpg) 
                + engine_size:curb_weight + curb_weight:city_mpg
                , auto_prices)

# evaluate
summary(ols_final)
```

Finally, lets test the residuals.

``` {r warning=FALSE}
# residual diagnostics
plot(ols_final)

# tests on residuals
residuals <- ols_final$residuals
residuals <- (residuals - mean(residuals)) / sd(residuals)

ks.test(residuals, pnorm)

shapiro.test(residuals)
```

Now both tests fail to reject null hypothesis, indicating residuals are likely normally distributed. Hence, validating our final model of:

`log(price) ~ log(engine_size) + sqrt(curb_weight) + log(city_mpg) + engine_size:curb_weight + curb_weight:city_mpg`